{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "fdQvjZgJSh4z",
    "outputId": "81923433-e2e5-407e-9ee8-92c369f3c8d5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "path = '' # path to your training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 67
    },
    "id": "8qr16eJ6ZMnZ",
    "outputId": "eaf09c18-a45d-409c-ff99-fa076dadfcf3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: fuzzywuzzy in /usr/local/lib/python3.6/dist-packages (0.18.0)\n",
      "Requirement already satisfied: python-Levenshtein in /usr/local/lib/python3.6/dist-packages (0.12.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from python-Levenshtein) (50.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install fuzzywuzzy\n",
    "!pip install python-Levenshtein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "HJcMfVcuSmlR"
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from nltk.corpus import stopwords\n",
    "from keras import Sequential\n",
    "from keras.layers import Embedding, Bidirectional, Dense, LSTM\n",
    "import nltk\n",
    "from fuzzywuzzy import fuzz\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "hhS6eqMXSw8u"
   },
   "outputs": [],
   "source": [
    "# params\n",
    "vocab_size = 50000\n",
    "embedding_dim = 64\n",
    "max_length = 50\n",
    "trunc_type = 'post'\n",
    "padding_type = 'post'\n",
    "oov_tok = '<OOV>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "ta6UOsr_YQeB"
   },
   "outputs": [],
   "source": [
    "# Classification labels \n",
    "POLARITY = [\"POSITIVE\", \"NEUTRAL\", \"NEGATIVE\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "1jb_YFmZZEm2"
   },
   "outputs": [],
   "source": [
    "sentences = []\n",
    "polarity_labels = []\n",
    "\n",
    "# Read and process the training data\n",
    "with open(path + \"training_data.txt\") as data:\n",
    "    reader = csv.reader(data, delimiter=\"\\t\")\n",
    "    for row in reader:        \n",
    "        sentences.append(row[1])\n",
    "        \n",
    "        for (index, label) in enumerate(POLARITY):\n",
    "            if fuzz.ratio(row[2].strip(), label) > 90:\n",
    "                polarity_labels.append(index)           \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "BErZTRtzS7Ez"
   },
   "outputs": [],
   "source": [
    "def text_to_seq(data):\n",
    "  tokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_tok)\n",
    "  tokenizer.fit_on_texts(data)\n",
    "  data_sequences = tokenizer.texts_to_sequences(data)\n",
    "  padded_sequences = pad_sequences(data_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
    "  return padded_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "o-74xTB0TvDS"
   },
   "outputs": [],
   "source": [
    "train_padded = text_to_seq(sentences)\n",
    "clipe_padded = text_to_seq(clipe_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "L4DcV38QT4Ad"
   },
   "outputs": [],
   "source": [
    "def get_label_seq(labels):\n",
    "  label_tokenizer = Tokenizer()\n",
    "  label_tokenizer.fit_on_texts(labels)\n",
    "\n",
    "  label_seq = np.array(label_tokenizer.texts_to_sequences(labels))\n",
    "  return label_seq - 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "Pbm-Xe8MVMUf"
   },
   "outputs": [],
   "source": [
    "def get_model(number_of_labels):\n",
    "  model = Sequential()\n",
    "  model.add(Embedding(vocab_size, embedding_dim))\n",
    "  model.add(Bidirectional(LSTM(embedding_dim)))\n",
    "  model.add(Dense(embedding_dim, activation='relu'))\n",
    "  model.add(Dense(number_of_labels, activation='softmax'))\n",
    "\n",
    "  return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x2d9IpywlfuE"
   },
   "source": [
    "### Sentiment Analysis using Bidirectional LSTM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 286
    },
    "id": "UD6-My1skL0s",
    "outputId": "13715766-cd86-42c1-dd09-5b17ac5ef331"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, None, 64)          3200000   \n",
      "_________________________________________________________________\n",
      "bidirectional_3 (Bidirection (None, 128)               66048     \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 3)                 195       \n",
      "=================================================================\n",
      "Total params: 3,274,499\n",
      "Trainable params: 3,274,499\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "polarity_model = get_model(len(POLARITY))\n",
    "polarity_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 151
    },
    "id": "ihnegRxhX1PW",
    "outputId": "bfe954b2-ea02-4133-c4bf-30bbd8c1da8e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "80/80 - 6s - loss: 0.9207 - accuracy: 0.4949\n",
      "Epoch 2/4\n",
      "80/80 - 6s - loss: 0.7236 - accuracy: 0.6559\n",
      "Epoch 3/4\n",
      "80/80 - 6s - loss: 0.3588 - accuracy: 0.8598\n",
      "Epoch 4/4\n",
      "80/80 - 6s - loss: 0.1593 - accuracy: 0.9465\n"
     ]
    }
   ],
   "source": [
    "polarity_model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "num_epochs = 4\n",
    "polarit_history = polarity_model.fit(train_padded, np.array(polarity_labels), epochs=num_epochs, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "id": "PzoHc3rmaXxP",
    "outputId": "f4db2d08-a6e2-4bd4-e864-27e7f193fffa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80/80 [==============================] - 1s 13ms/step - loss: 0.0998 - accuracy: 0.9711\n",
      "97.10937738418579\n"
     ]
    }
   ],
   "source": [
    "loss, acc = polarity_model.evaluate(train_padded, np.array(polarity_labels))\n",
    "print(acc * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "utXIr5vol0GM",
    "outputId": "60f63947-40ee-4019-bdb5-d7fd71233317"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /content/drive/My Drive/NLPHW/PS2/polarity/assets\n"
     ]
    }
   ],
   "source": [
    "polarity_model.save(path + 'polarity/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dVYv7Vkcl2v5"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "LSTM.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
