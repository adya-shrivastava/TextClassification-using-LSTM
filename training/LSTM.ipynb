{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LSTM.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "fdQvjZgJSh4z",
        "outputId": "81923433-e2e5-407e-9ee8-92c369f3c8d5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "path = '' # path to your training data"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8qr16eJ6ZMnZ",
        "outputId": "eaf09c18-a45d-409c-ff99-fa076dadfcf3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "!pip install fuzzywuzzy\n",
        "!pip install python-Levenshtein"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: fuzzywuzzy in /usr/local/lib/python3.6/dist-packages (0.18.0)\n",
            "Requirement already satisfied: python-Levenshtein in /usr/local/lib/python3.6/dist-packages (0.12.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from python-Levenshtein) (50.3.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HJcMfVcuSmlR"
      },
      "source": [
        "import csv\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from nltk.corpus import stopwords\n",
        "from keras import Sequential\n",
        "from keras.layers import Embedding, Bidirectional, Dense, LSTM\n",
        "import nltk\n",
        "from fuzzywuzzy import fuzz\n"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hhS6eqMXSw8u"
      },
      "source": [
        "# params\n",
        "vocab_size = 50000\n",
        "embedding_dim = 64\n",
        "max_length = 50\n",
        "trunc_type = 'post'\n",
        "padding_type = 'post'\n",
        "oov_tok = '<OOV>'"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ta6UOsr_YQeB"
      },
      "source": [
        "# Classification labels \n",
        "POLARITY = [\"POSITIVE\", \"NEUTRAL\", \"NEGATIVE\"]"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1jb_YFmZZEm2"
      },
      "source": [
        "sentences = []\n",
        "polarity_labels = []\n",
        "\n",
        "# Read and process the training data\n",
        "with open(path + \"training_data.txt\") as data:\n",
        "    reader = csv.reader(data, delimiter=\"\\t\")\n",
        "    for row in reader:        \n",
        "        sentences.append(row[1])\n",
        "        \n",
        "        for (index, label) in enumerate(POLARITY):\n",
        "            if fuzz.ratio(row[2].strip(), label) > 90:\n",
        "                polarity_labels.append(index)           \n"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BErZTRtzS7Ez"
      },
      "source": [
        "def text_to_seq(data):\n",
        "  tokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_tok)\n",
        "  tokenizer.fit_on_texts(data)\n",
        "  data_sequences = tokenizer.texts_to_sequences(data)\n",
        "  padded_sequences = pad_sequences(data_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
        "  return padded_sequences"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o-74xTB0TvDS"
      },
      "source": [
        "train_padded = text_to_seq(sentences)\n",
        "clipe_padded = text_to_seq(clipe_sentences)"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L4DcV38QT4Ad"
      },
      "source": [
        "def get_label_seq(labels):\n",
        "  label_tokenizer = Tokenizer()\n",
        "  label_tokenizer.fit_on_texts(labels)\n",
        "\n",
        "  label_seq = np.array(label_tokenizer.texts_to_sequences(labels))\n",
        "  return label_seq - 1\n"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pbm-Xe8MVMUf"
      },
      "source": [
        "def get_model(number_of_labels):\n",
        "  model = Sequential()\n",
        "  model.add(Embedding(vocab_size, embedding_dim))\n",
        "  model.add(Bidirectional(LSTM(embedding_dim)))\n",
        "  model.add(Dense(embedding_dim, activation='relu'))\n",
        "  model.add(Dense(number_of_labels, activation='softmax'))\n",
        "\n",
        "  return model"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x2d9IpywlfuE"
      },
      "source": [
        "### Sentiment Analysis using Bidirectional LSTM "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UD6-My1skL0s",
        "outputId": "13715766-cd86-42c1-dd09-5b17ac5ef331",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        }
      },
      "source": [
        "polarity_model = get_model(len(POLARITY))\n",
        "polarity_model.summary()"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_3 (Embedding)      (None, None, 64)          3200000   \n",
            "_________________________________________________________________\n",
            "bidirectional_3 (Bidirection (None, 128)               66048     \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 64)                8256      \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 3)                 195       \n",
            "=================================================================\n",
            "Total params: 3,274,499\n",
            "Trainable params: 3,274,499\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ihnegRxhX1PW",
        "outputId": "bfe954b2-ea02-4133-c4bf-30bbd8c1da8e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 151
        }
      },
      "source": [
        "polarity_model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "num_epochs = 4\n",
        "polarit_history = polarity_model.fit(train_padded, np.array(polarity_labels), epochs=num_epochs, verbose=2)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/4\n",
            "80/80 - 6s - loss: 0.9207 - accuracy: 0.4949\n",
            "Epoch 2/4\n",
            "80/80 - 6s - loss: 0.7236 - accuracy: 0.6559\n",
            "Epoch 3/4\n",
            "80/80 - 6s - loss: 0.3588 - accuracy: 0.8598\n",
            "Epoch 4/4\n",
            "80/80 - 6s - loss: 0.1593 - accuracy: 0.9465\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PzoHc3rmaXxP",
        "outputId": "f4db2d08-a6e2-4bd4-e864-27e7f193fffa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "loss, acc = polarity_model.evaluate(train_padded, np.array(polarity_labels))\n",
        "print(acc * 100)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "80/80 [==============================] - 1s 13ms/step - loss: 0.0998 - accuracy: 0.9711\n",
            "97.10937738418579\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "utXIr5vol0GM",
        "outputId": "60f63947-40ee-4019-bdb5-d7fd71233317",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "polarity_model.save(path + 'polarity/')"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/NLPHW/PS2/polarity/assets\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dVYv7Vkcl2v5"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}